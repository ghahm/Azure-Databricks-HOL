file_location = "abfss://demodata@ghadlskrc.dfs.core.windows.net/sparkhol/flight-data/flights.csv"
file_type = "csv"

# COMMAND ----------

flightDF = spark.read.format(file_type).option("inferSchema", "true").load(file_location)
display(flightDF)# ADLS 정상 조회 테스트

# COMMAND ----------

# 컬럼명 변경
renamedflightDF = flightDF.withColumnRenamed("_c0", "Month") \
                            .withColumnRenamed("_c1", "DayofMonth") \
                            .withColumnRenamed("_c1", "DayofMonth") \
                            .withColumnRenamed("_c2", "DayOfWeek") \
                            .withColumnRenamed("_c3", "DepTime") \
                            .withColumnRenamed("_c4", "ArrTime") \
                            .withColumnRenamed("_c5", "UniqueCarrier") \
                            .withColumnRenamed("_c6", "FlightNum") \
                            .withColumnRenamed("_c7", "TailNum") \
                            .withColumnRenamed("_c8", "ElapsedTime") \
                            .withColumnRenamed("_c9", "AirTime") \
                            .withColumnRenamed("_c10", "ArrDelay") \
                            .withColumnRenamed("_c11", "DepDelay") \
                            .withColumnRenamed("_c12", "Origin") \
                            .withColumnRenamed("_c13", "Dest") \
                            .withColumnRenamed("_c14", "Distance") \
                            .withColumnRenamed("_c15", "TaxiIn") \
                            .withColumnRenamed("_c16", "TaxiOut") \
                            .withColumnRenamed("_c17", "Cancelled") \
                            .withColumnRenamed("_c18", "CancellationCode") \
                            .withColumnRenamed("_c19", "Diverted")
renamedflightDF.printSchema()

# COMMAND ----------

# 컬럼 타입 수정
castedflightDF = renamedflightDF.withColumn("Month", renamedflightDF.Month.cast('string')) \
                                .withColumn("DayofMonth", renamedflightDF.DayofMonth.cast('string')) \
                                .withColumn("DayOfWeek", renamedflightDF.DayOfWeek.cast('string')) \
                                .withColumn("FlightNum", renamedflightDF.FlightNum.cast('string'))

castedflightDF.printSchema()

# COMMAND ----------

# 쿼리 수행을 위하여 DataFrame을 임시 테이블로 등록
castedflightDF.createOrReplaceTempView("flightTable")

# COMMAND ----------

%sql
show tables

# COMMAND ----------

castedflightDF.write.saveAsTable("flightDSTable")

# COMMAND ----------

%sql
show tables

# COMMAND ----------

# 1.3 sqlContext를 이용하여 쿼리 수행 (주의 : 코드를 Copy하면 쿼리 사이에 빈 라인이 생겨 에러 발생. 해당 빈 라인을 삭제할 )
resultDF = sqlContext.sql("SELECT UniqueCarrier, count(UniqueCarrier) as cnt \
                        FROM flightTable \
                        GROUP BY UniqueCarrier \
                        ORDER BY cnt DESC")
resultDF.collect()

# COMMAND ----------

# 1.4 %sql magic 정의 -> 쿼리 수행 
# (주의 : 아래 sql magic이 포함된 셀에는 # comment를 추가하면 에러 발생하기 때문에 별도의 셀로 작성)
# (주의 : 아래 sql 앞의 %는 HDI Spark에서는 %%로 Databricks에서는 %로 할 )

# COMMAND ----------

%sql
SELECT UniqueCarrier, count(UniqueCarrier) as cnt 
FROM flightTable
GROUP BY UniqueCarrier
ORDER BY cnt DESC

# COMMAND ----------

%sql
SELECT UniqueCarrier, count(UniqueCarrier) as cnt 
FROM flightDSTable
GROUP BY UniqueCarrier
ORDER BY cnt DESC

# COMMAND ----------

%sql
DROP TABLE flightDSTable

# COMMAND ----------

%sql
show tables



